{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b53b97c6-ff78-4b61-9155-4ae2b26b6299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb0394a3-4a3a-4d9c-b3b5-79de88a7527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Parameters\n",
    "time_seires_num = 30\n",
    "country_you_want_to_predict = \"china\"\n",
    "case_you_want_to_predict = \"confirmed\"\n",
    "prediction_date = [\"2022-09-01\", \"2022-09-02\", \"2022-09-03\", \"2022-09-04\", \"2022-09-05\"] # The date should not exceed 2023-03-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55ce1523-74ad-44d9-a0f1-f3b485eb1c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Check if a SparkContext exists, if not, create one\n",
    "try:\n",
    "    sc = SparkContext.getOrCreate()\n",
    "except:\n",
    "    sc = SparkContext(\"local\", \"ARIMA\")\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession(sc)\n",
    "df = spark.read.csv('1.csv', header=True, inferSchema=True)\n",
    "df = df.filter(df['country']==country_you_want_to_predict).select(\"date\", case_you_want_to_predict)\n",
    "df = df.withColumnRenamed(case_you_want_to_predict, \"cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f330ede9-016b-4c65-9e86-7af9939d25f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f24911c6-651b-4231-bcc5-0be16ffba286",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_function' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m error\n\u001b[1;32m     38\u001b[0m rdd \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mparallelize(prediction_date, \u001b[38;5;28mlen\u001b[39m(prediction_date))\n\u001b[0;32m---> 39\u001b[0m result_rdd \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmap(\u001b[43mmy_function\u001b[49m)\n\u001b[1;32m     40\u001b[0m final_result \u001b[38;5;241m=\u001b[39m result_rdd\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(final_result)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'my_function' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def predict_time_series_arima(prediction_date):\n",
    "    data = df\n",
    "    # Calculate the difference\n",
    "    prediction_date_dt = datetime.strptime(prediction_date, \"%Y-%m-%d\")\n",
    "    first_date_in_training_set = prediction_date_dt - timedelta(days=time_seires_num)\n",
    "    last_date_in_training_set = prediction_date_dt - timedelta(days=1)\n",
    "\n",
    "    # Get the true data\n",
    "    true_value = data.filter(col(\"date\") == prediction_date).select(\"cases\").collect()[0][\"cases\"]\n",
    "    \n",
    "    # Filter the date in data\n",
    "    data = data.filter((col(\"date\")>=first_date_in_training_set) & (col(\"date\")<=last_date_in_training_set))\n",
    "    \n",
    "    # Assemble features\n",
    "    assembler = VectorAssembler(inputCols=[\"cases\"], outputCol=\"features\")\n",
    "    assembled_data = assembler.transform(data)\n",
    "\n",
    "    # Extract features as a NumPy array\n",
    "    np_data = np.array(assembled_data.select(\"features\").rdd.map(lambda x: x[0].toArray()[0]).collect())\n",
    "\n",
    "    # Fit an ARIMA model\n",
    "    order = (1, 1, 1)  # Example order, you may need to tune this based on your data\n",
    "    model = ARIMA(np_data, order=order)\n",
    "    fit_model = model.fit()\n",
    "\n",
    "    prediction_value = int(fit_model.forecast(1)[0])\n",
    "    error = abs(prediction_value - true_value)/true_value*100\n",
    "    print(f\"Predicted value for {prediction_date}: {prediction_value}; True value for {prediction_date}: {true_value}; The error is: {error}%\")\n",
    "    return error\n",
    "\n",
    "rdd = sc.parallelize(prediction_date, len(prediction_date))\n",
    "result_rdd = rdd.map(my_function(date))\n",
    "final_result = result_rdd.collect()\n",
    "print(final_result)\n",
    "#predict_time_series_arima(spark, df, prediction_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ab2205-767a-416f-b5d6-f171f6aabb42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
