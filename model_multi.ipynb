{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8acb0e20-0d64-4b2e-afaf-00dbccab2ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: statsmodels in /home/hadoop/.local/lib/python3.10/site-packages (0.14.0)\n",
      "Requirement already satisfied: pandas>=1.0 in /home/hadoop/.local/lib/python3.10/site-packages (from statsmodels) (2.1.3)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /home/hadoop/.local/lib/python3.10/site-packages (from statsmodels) (0.5.3)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.4 in /home/hadoop/.local/lib/python3.10/site-packages (from statsmodels) (1.11.4)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/hadoop/.local/lib/python3.10/site-packages (from statsmodels) (23.2)\n",
      "Requirement already satisfied: numpy>=1.18 in /home/hadoop/.local/lib/python3.10/site-packages (from statsmodels) (1.26.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=1.0->statsmodels) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/hadoop/.local/lib/python3.10/site-packages (from pandas>=1.0->statsmodels) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/hadoop/.local/lib/python3.10/site-packages (from pandas>=1.0->statsmodels) (2023.3)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from patsy>=0.5.2->statsmodels) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in /home/hadoop/.local/lib/python3.10/site-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/hadoop/.local/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/hadoop/.local/lib/python3.10/site-packages (1.26.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install statsmodels\n",
    "!pip install pyspark\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b53b97c6-ff78-4b61-9155-4ae2b26b6299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from pyspark.sql.functions import col\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "833b7827-ed72-4e08-a274-d1e9a6d58162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/30 11:50:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "sc = pyspark.SparkContext(master='spark://hd-master:7077',\n",
    "                          appName='big_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33f4734c-0b35-4ee9-931b-1c6e52c58d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/.local/lib/python3.10/site-packages/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "spark = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c24eacf-544d-4eaf-985c-f880cc2c3b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\",True).csv(\"hdfs://hd-master:9000/covid-19\", inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "982cb5bd-70c9-4bbe-9f11-44013c1c889f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+---------+------+\n",
      "|country    |date      |confirmed|recovered|deaths|\n",
      "+-----------+----------+---------+---------+------+\n",
      "|afghanistan|2020-01-22|0        |0        |0     |\n",
      "|afghanistan|2020-01-23|0        |0        |0     |\n",
      "|afghanistan|2020-01-24|0        |0        |0     |\n",
      "|afghanistan|2020-01-25|0        |0        |0     |\n",
      "|afghanistan|2020-01-26|0        |0        |0     |\n",
      "+-----------+----------+---------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb0394a3-4a3a-4d9c-b3b5-79de88a7527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Parameters\n",
    "time_seires_num = 30\n",
    "country_you_want_to_predict = \"china\"\n",
    "case_you_want_to_predict = \"confirmed\"\n",
    "prediction_date = \"2022-09-01\" # The date should not exceed 2023-03-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55ce1523-74ad-44d9-a0f1-f3b485eb1c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df['country']==country_you_want_to_predict).select(\"date\", case_you_want_to_predict)\n",
    "df = df.withColumnRenamed(case_you_want_to_predict, \"cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a4a1363-4e6b-4c6f-8716-35ea9c53aa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|date      |cases|\n",
      "+----------+-----+\n",
      "|2020-01-22|548  |\n",
      "|2020-01-23|643  |\n",
      "|2020-01-24|920  |\n",
      "|2020-01-25|1406 |\n",
      "|2020-01-26|2075 |\n",
      "+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f24911c6-651b-4231-bcc5-0be16ffba286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_time_series_arima(prediction_date):\n",
    "    data = df\n",
    "    # Calculate the difference\n",
    "    prediction_date_dt = datetime.strptime(prediction_date, \"%Y-%m-%d\")\n",
    "    first_date_in_training_set = prediction_date_dt - timedelta(days=time_seires_num)\n",
    "    last_date_in_training_set = prediction_date_dt - timedelta(days=1)\n",
    "\n",
    "    # Get the true data\n",
    "    true_value = data.filter(col(\"date\") == prediction_date).select(\"cases\").collect()[0][\"cases\"]\n",
    "    \n",
    "    # Filter the date in data\n",
    "    data = data.filter((col(\"date\")>=first_date_in_training_set) & (col(\"date\")<=last_date_in_training_set))\n",
    "    \n",
    "    # Assemble features\n",
    "    assembler = VectorAssembler(inputCols=[\"cases\"], outputCol=\"features\")\n",
    "    assembled_data = assembler.transform(data)\n",
    "\n",
    "    # Extract features as a NumPy array\n",
    "    np_data = np.array(assembled_data.select(\"features\").rdd.map(lambda x: x[0].toArray()[0]).collect())\n",
    "\n",
    "    # Fit an ARIMA model\n",
    "    order = (1, 1, 1)  # Example order, you may need to tune this based on your data\n",
    "    model = ARIMA(np_data, order=order)\n",
    "    fit_model = model.fit()\n",
    "\n",
    "    prediction_value = int(fit_model.forecast(1)[0])\n",
    "    error = abs(prediction_value - true_value)/true_value*100\n",
    "    print(f\"Predicted value for {prediction_date}: {prediction_value}; True value for {prediction_date}: {true_value}; The error is: {error}%\")\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ab2205-767a-416f-b5d6-f171f6aabb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value for 2022-09-01: 2505900; True value for 2022-09-01: 2510703; The error is: 0.19130100214959714%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/.local/lib/python3.10/site-packages/statsmodels/tsa/statespace/sarimax.py:966: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-stationary starting autoregressive parameters'\n",
      "/home/hadoop/.local/lib/python3.10/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.19130100214959714"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_time_series_arima(prediction_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e77e75c-3a95-4ad4-ba5e-85da6e36648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the list of dates for prediction\n",
    "date_list = [\"2022-09-01\", \"2022-10-01\", \"2022-11-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c450961b-ebae-4af3-8701-23a6a5c921a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/.local/lib/python3.10/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value for 2022-09-01: 2505900; True value for 2022-09-01: 2510703; The error is: 0.19130100214959714%\n",
      "0.19130100214959714\n",
      "Predicted value for 2022-10-01: 2762240; True value for 2022-10-01: 2762150; The error is: 0.0032583313723005634%\n",
      "0.0032583313723005634\n",
      "Predicted value for 2022-11-01: 2958628; True value for 2022-11-01: 2959481; The error is: 0.02882262126366076%\n",
      "0.02882262126366076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/.local/lib/python3.10/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    }
   ],
   "source": [
    "for date in date_list:\n",
    "    err = predict_time_series_arima(date)\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea49b10-1d65-4e77-84fa-dcc0427dbee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
