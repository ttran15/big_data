{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b53b97c6-ff78-4b61-9155-4ae2b26b6299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb0394a3-4a3a-4d9c-b3b5-79de88a7527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Parameters\n",
    "time_seires_num = 30\n",
    "country_you_want_to_predict = \"china\"\n",
    "case_you_want_to_predict = \"confirmed\"\n",
    "prediction_date = \"2022-09-01\" # The date should not exceed 2023-03-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ce1523-74ad-44d9-a0f1-f3b485eb1c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Check if a SparkContext exists, if not, create one\n",
    "try:\n",
    "    sc = SparkContext.getOrCreate()\n",
    "except:\n",
    "    sc = SparkContext(\"local\", \"ARIMA\")\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession(sc)\n",
    "df = spark.read.csv('covid-19.csv', header=True, inferSchema=True)\n",
    "df = df.filter(df['country']==country_you_want_to_predict).select(\"date\", case_you_want_to_predict)\n",
    "df = df.withColumnRenamed(case_you_want_to_predict, \"cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f330ede9-016b-4c65-9e86-7af9939d25f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f24911c6-651b-4231-bcc5-0be16ffba286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def predict_time_series_arima(prediction_date):\n",
    "    data = df\n",
    "    # Calculate the difference\n",
    "    prediction_date_dt = datetime.strptime(prediction_date, \"%Y-%m-%d\")\n",
    "    first_date_in_training_set = prediction_date_dt - timedelta(days=time_seires_num)\n",
    "    last_date_in_training_set = prediction_date_dt - timedelta(days=1)\n",
    "\n",
    "    # Get the true data\n",
    "    true_value = data.filter(col(\"date\") == prediction_date).select(\"cases\").collect()[0][\"cases\"]\n",
    "    \n",
    "    # Filter the date in data\n",
    "    data = data.filter((col(\"date\")>=first_date_in_training_set) & (col(\"date\")<=last_date_in_training_set))\n",
    "    \n",
    "    # Assemble features\n",
    "    assembler = VectorAssembler(inputCols=[\"cases\"], outputCol=\"features\")\n",
    "    assembled_data = assembler.transform(data)\n",
    "\n",
    "    # Extract features as a NumPy array\n",
    "    np_data = np.array(assembled_data.select(\"features\").rdd.map(lambda x: x[0].toArray()[0]).collect())\n",
    "\n",
    "    # Fit an ARIMA model\n",
    "    order = (1, 1, 1)  # Example order, you may need to tune this based on your data\n",
    "    model = ARIMA(np_data, order=order)\n",
    "    fit_model = model.fit()\n",
    "\n",
    "    prediction_value = int(fit_model.forecast(1)[0])\n",
    "    error = abs(prediction_value - true_value)/true_value*100\n",
    "    print(f\"Predicted value for {prediction_date}: {prediction_value}; True value for {prediction_date}: {true_value}; The error is: {error}%\")\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ab2205-767a-416f-b5d6-f171f6aabb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value for 2022-09-01: 2505900; True value for 2022-09-01: 2510703; The error is: 0.19130100214959714%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/statsmodels/tsa/statespace/sarimax.py:966: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-stationary starting autoregressive parameters'\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.19130100214959714"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_time_series_arima(prediction_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e77e75c-3a95-4ad4-ba5e-85da6e36648b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
